from chardet.universaldetector import UniversalDetector
from datetime import date
import pathlib
import re
import csv
import os

def identify_file_suffix(filename):
    file_extension_type = pathlib.Path(str(filename)).suffix
    
    if file_extension_type == '.csv':
        return 'CSV'
    elif file_extension_type == '.txt':
        return 'TXT'
    elif file_extension_type == '.sql':
        return 'SQL'
    elif file_extension_type == '.json':
        return 'JSON'

def identify_hash_type(hash_string):
    """Identifies the type of a hash based on its length."""
    hash_length = len(hash_string)

    if hash_length == 32 or hash_length == 35:
        md5_hash_pattern = r'\b([a-f0-9]{32})\b'
        matches = re.findall(md5_hash_pattern, hash_string)
        
        if matches:
            return 'MD5'

    elif hash_length == 40:
        return "SHA-1"
    elif hash_length == 64:
        return "SHA-256"
    elif hash_length == 128:
        return "SHA-512"
    else:
        return "Unknown"

def identify_encoding(filename):
    detector = UniversalDetector()
    detector.reset()
    
    for line in open(filename, 'rb'):
        detector.feed(line)
        if detector.done: break
    
    detector.close()
    
    return detector.result['encoding']

def identify_password(line):
    # To-do: Create a better regex for finding passwords
    password_pattern = r'(?=.*[A-Z])(?=.*[a-z])(?=.*[0-9])(?=.*[^a-zA-Z0-9])(?!.*@)([A-Za-z0-9!@#$%^&*()_+={}\[\]:;"\'<>,.?/~`-]{6,})'

    password_match = re.search(password_pattern, line)
    if password_match:
        password = str(password_match.group(1))
        return password 

def parsing_lines(line, file_suffix, data):
    hash_types = {
        'MD5':      'found',
        'SHA-1':    'found',
        'SHA-256':  'found',
        'SHA-512':  'found'
    }
    email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b'
    
    pattern_match = re.search(email_pattern, line)
    
    if file_suffix == 'TXT':
        if pattern_match:
            email = str(pattern_match.group())
            password = identify_password(line=line)
            
            if email not in data:
                data[email] = {"passwords": [], "line": None}

            if password not in data[email]['passwords']:
                data[email]['passwords'].append(password)
            
            data[email]["line"] = str(line)

            return data

    elif file_suffix == 'SQL':
        hash_data = line.strip().split(",")
        for hash_string in hash_data:
            hash_type = identify_hash_type(hash_string)
            if pattern_match and hash_types.get(hash_type) == 'found':
                email = str(pattern_match.group())
                
                if email not in data:
                    data[email] = {"hashes": set(), "line": None} 

                data[email]["hashes"].add(hash_string.replace("'", ''))
                data[email]["line"] = str(line)

def generate_file_name():
    today = date.today()
    base_name = f"{today}-dataset"
    extension = ".csv"

    file_name = f"{base_name}{extension}"
    counter = 0

    while os.path.exists(file_name):
        file_name = f"{base_name}{counter}{extension}"
        
        counter += 1

    return file_name

def write_data_set(data):
    for email, hashes in data.items():
        string = ''.join(hashes['hashes'])
        # print(string[1:].replace(" ", '-'))
    filename = generate_file_name()
    with open(filename, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(['email', 'hashes', 'line'])

        for email, hashes in data.items():
            hash_value = ''.join(hashes['hashes'])
            writer.writerow([email, hash_value[1:].replace(" ", '-'), hashes['line']]) 

def reading_files(filename):
    data = {}
    encoder = identify_encoding(filename=filename)
    file_suffix = identify_file_suffix(filename=filename)
    
    count = 0

    with open(str(filename), 'r', encoding=str(encoder)) as file:
        lines = file.readlines()
        for line in lines:
            parsing_lines(line=line, file_suffix=file_suffix, data=data)
            count += 1

            if count == 100:
                break


    write_data_set(data)
